{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bdaa1f",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c621fd",
   "metadata": {},
   "source": [
    "# 13th exercise: <font color=\"#C70039\">First Reinforcement Learning Q-Table learning</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "    * Date:   01.04.2026\n",
    "    * Student: Tim Voßmerbäumer\n",
    "    * Matr.Nr.: 11474232\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. play with all hyperparameters including the actions, states, rewards table.\n",
    "5. add and implement an ϵ-greedy strategy \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b5efe-2b35-4935-af30-8c270bd3b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d567ab5-06ac-418f-96ca-892695bf9f61",
   "metadata": {},
   "source": [
    "### Create the possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d53fc92-527b-46c6-bc57-4e68423aa61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8\n",
    "}\n",
    "\n",
    "state_to_location = dict((state,location) for location, state in location_to_state.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3fd99",
   "metadata": {},
   "source": [
    "### Create the actions & rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349627df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac22125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                   [1,0,1,0,1,0,0,0,0],\n",
    "                   [0,1,0,0,0,1,0,0,0],\n",
    "                   [0,0,0,0,0,0,1,0,0],\n",
    "                   [0,1,0,0,0,0,0,1,0],\n",
    "                   [0,0,1,0,0,0,0,0,0],\n",
    "                   [0,0,0,1,0,0,0,1,0],\n",
    "                   [0,0,0,0,1,0,1,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a644-6339-4f56-8066-9dd3d629190a",
   "metadata": {},
   "source": [
    "### Def remaining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f112bf-e2e1-42a1-a9b3-df514af058c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "gamma = 0.99 # discount factor\n",
    "alpha = 0.7  # learning rate\n",
    "epsilon = 0.1 # Higher value encourages more exploration, lower value encourages more exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37973f00-e730-4fd2-b660-0ff268984e61",
   "metadata": {},
   "source": [
    "### Define agent and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a8c7ba-023b-414c-b007-64e789dcd9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    # initialize everything\n",
    "    def __init__(self, alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        # remember, the Q-value table is of size all actions x all states\n",
    "        M = len(location_to_state)\n",
    "        self.Q = np.zeros((M,M), dtype = None, order = 'C')\n",
    "        \n",
    "    # now, implement the training method for the agent\n",
    "    def training(self, start_location, end_location, iterations):\n",
    "\n",
    "        rewards_new = np.copy(self.rewards)\n",
    "\n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        \n",
    "        rewards_new[ending_state, ending_state] = 999\n",
    "        \n",
    "        # DEBUG\n",
    "        print(rewards_new)\n",
    "\n",
    "        # pick random current state\n",
    "        # iterations = the # of training cycles\n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0,9)\n",
    "            playable_actions = []\n",
    "\n",
    "            # iterate thru the rewards matrix to get states\n",
    "            # that are really reachable from the randomly chosen\n",
    "            # state and assign only those in a list since they are really playable\n",
    "            for j in range(9):\n",
    "                if rewards_new[current_state, j] > 0:\n",
    "                    playable_actions.append(j)\n",
    "\n",
    "            # choosing next random state using epsilon-greedy strategy\n",
    "            if np.random.uniform(0, 1) < self.epsilon:\n",
    "                # Explore: choose a random action from playable_actions\n",
    "                if len(playable_actions) != 0:\n",
    "                    next_state = np.random.choice(playable_actions)\n",
    "                else:\n",
    "                    # If no playable actions, stay in current state (or handle as an edge case)\n",
    "                    next_state = current_state\n",
    "            else:\n",
    "                # Exploit: choose the action with the highest Q-value\n",
    "                # If there are no playable actions from current_state, Q[current_state, :] will be all zeros.\n",
    "                # In such a case, argmax will return 0. We need to ensure we only pick valid actions.\n",
    "                q_values_for_current_state = self.Q[current_state, :]\n",
    "                valid_q_values = [q_values_for_current_state[action] for action in playable_actions]\n",
    "\n",
    "                if valid_q_values:\n",
    "                    # Get the index of the max Q-value among playable actions\n",
    "                    max_q_index = np.argmax(valid_q_values)\n",
    "                    # Map this index back to the actual state in playable_actions\n",
    "                    next_state = playable_actions[max_q_index]\n",
    "                else:\n",
    "                    # Fallback if no valid actions (should ideally not happen with proper environment)\n",
    "                    next_state = current_state\n",
    "    \n",
    "            # finding the difference in Q, often referred to as temporal difference\n",
    "            # by means of the Bellman's equation (compare with slides)\n",
    "            TD = rewards_new[current_state, next_state] + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state, next_state]\n",
    "            self.Q[current_state, next_state] += self.alpha*TD\n",
    "            # DEBUG\n",
    "            #print(f\"Q[{current_state}, {next_state}]:\", self.Q[current_state, next_state])\n",
    "\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "\n",
    "        # print the optimal route from start to end\n",
    "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "\n",
    "    # compute the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        while(next_location != end_location):\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "        # DEBUG\n",
    "        print('Q-table:',Q)\n",
    "        print(\"optimal route:\", route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bc2a1b-4291-4560-ba23-0dc145e01154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[0.00000000e+00 5.26779245e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.69325569e+01 0.00000000e+00 5.22599109e+01 0.00000000e+00\n",
      "  2.14877412e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.27350248e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.31268007e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.57360853e+04\n",
      "  0.00000000e+00 0.00000000e+00 2.54732734e+01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.27184300e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.05825206e+01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.25334969e+01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.48671782e+04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.39395102e+01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.80335510e+01 0.00000000e+00 3.43243509e+04 0.00000000e+00\n",
      "  3.86767901e+01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.39170366e+04\n",
      "  0.00000000e+00]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "qagent = QAgent(alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location)\n",
    "qagent.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cf545",
   "metadata": {},
   "source": [
    "## Hyperparameter Exploration and Epsilon-Greedy Strategy\n",
    "\n",
    "Below are examples of how you can experiment with the hyperparameters and the newly implemented epsilon-greedy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6579e5",
   "metadata": {},
   "source": [
    "### Experimenting with `epsilon`:\n",
    "A higher `epsilon` value encourages more exploration, which can be beneficial in complex environments to discover new paths, but might slow down convergence. A lower `epsilon` emphasizes exploitation (choosing the best-known action), leading to faster convergence once good paths are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c232bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Epsilon = 0.5 (More Exploration) ---\n",
      "[[  0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[    0.         38325.45766693     0.             0.\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [37245.4865412      0.         37644.0535499      0.\n",
      "  38722.62123183     0.             0.             0.\n",
      "      0.        ]\n",
      " [    0.         38263.12866246     0.             0.\n",
      "      0.         35423.18387945     0.             0.\n",
      "      0.        ]\n",
      " [    0.             0.             0.         42948.47367375\n",
      "      0.             0.         39278.36071109     0.\n",
      "      0.        ]\n",
      " [    0.         36736.02601753     0.             0.\n",
      "      0.             0.             0.         39114.82226284\n",
      "      0.        ]\n",
      " [    0.             0.         37877.34293144     0.\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [    0.             0.             0.         39916.06554774\n",
      "      0.             0.             0.         39065.53944005\n",
      "      0.        ]\n",
      " [    0.             0.             0.             0.\n",
      "  38529.80744439     0.         39517.79671968     0.\n",
      "  38712.61095701]\n",
      " [    0.             0.             0.             0.\n",
      "      0.             0.             0.         39116.80210298\n",
      "      0.        ]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "# Create a new QAgent instance with a higher epsilon for more exploration\n",
    "qagent_explore = QAgent(alpha, gamma, 0.5, location_to_state, actions, rewards, state_to_location)\n",
    "print(\"\\n--- Training with Epsilon = 0.5 (More Exploration) ---\")\n",
    "qagent_explore.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4117e5b",
   "metadata": {},
   "source": [
    "### Experimenting with `alpha` (learning rate):\n",
    "`alpha` determines how much new information overrides old information. A higher `alpha` means the agent learns more quickly from recent experiences, while a lower `alpha` makes learning slower but potentially more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ec8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Alpha = 0.1 (Slower Learning) ---\n",
      "[[  0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[0.00000000e+00 6.00679914e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.15999147e-01 0.00000000e+00 5.71452384e+01 0.00000000e+00\n",
      "  6.87110901e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 6.11665617e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.21480067e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.96230520e+03\n",
      "  0.00000000e+00 0.00000000e+00 1.81971258e+03 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 8.94040645e+02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.74052219e+03\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.03610635e+03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.36812595e+03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.39846568e+03\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.01398616e+02 0.00000000e+00 8.68533982e+03 0.00000000e+00\n",
      "  1.01731445e+03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.71121857e+03\n",
      "  0.00000000e+00]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "# Create a new QAgent instance with a lower alpha for slower, more stable learning\n",
    "qagent_slow_learn = QAgent(0.1, gamma, epsilon, location_to_state, actions, rewards, state_to_location)\n",
    "print(\"\\n--- Training with Alpha = 0.1 (Slower Learning) ---\")\n",
    "qagent_slow_learn.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7fc39",
   "metadata": {},
   "source": [
    "### Experimenting with `gamma` (discount factor):\n",
    "`gamma` controls the importance of future rewards. A `gamma` closer to 1 makes the agent consider long-term rewards more heavily, suitable for environments where delayed rewards are significant. A `gamma` closer to 0 makes the agent focus on immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11380d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Gamma = 0.5 (More Immediate Rewards Focus) ---\n",
      "[[  0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[0.00000000e+00 6.43750000e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.00000000e+00 0.00000000e+00 1.94365451e+00 0.00000000e+00\n",
      "  1.26750000e+02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.40000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.99800000e+03\n",
      "  0.00000000e+00 0.00000000e+00 5.00999855e+02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.87612471e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.51500000e+02\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.51317249e+02\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.25739097e+02 0.00000000e+00 5.01000000e+02 0.00000000e+00\n",
      "  1.25715522e+02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.51500000e+02\n",
      "  0.00000000e+00]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "# Create a new QAgent instance with a lower gamma for more immediate rewards focus\n",
    "qagent_immediate_rewards = QAgent(alpha, 0.5, epsilon, location_to_state, actions, rewards, state_to_location)\n",
    "print(\"\\n--- Training with Gamma = 0.5 (More Immediate Rewards Focus) ---\")\n",
    "qagent_immediate_rewards.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make the path from L1 to L2 more rewarding initially\n",
    "rewards_modified = np.copy(rewards)\n",
    "rewards_modified[0, 1] = 5 # L1 to L2 now has a higher initial reward\n",
    "rewards_modified[1, 0] = 20 # And from L2 to L1 now counts as goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6054763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Modified Rewards (L1-L2 more rewarding) ---\n",
      "[[  0   5   0   0   0   0   0   0   0]\n",
      " [ 20   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[    0.         45771.59750815     0.             0.\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [33926.31462309     0.             0.             0.\n",
      "  46314.80912919     0.             0.             0.\n",
      "      0.        ]\n",
      " [    0.         45844.75776808     0.             0.\n",
      "      0.         39454.8441217      0.             0.\n",
      "      0.        ]\n",
      " [    0.             0.             0.         50064.23958807\n",
      "      0.             0.         42735.56208539     0.\n",
      "      0.        ]\n",
      " [    0.         26649.68399155     0.             0.\n",
      "      0.             0.             0.         46806.41643658\n",
      "      0.        ]\n",
      " [    0.             0.         45316.57227056     0.\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [    0.             0.             0.         49151.72209857\n",
      "      0.             0.             0.         45639.40267375\n",
      "      0.        ]\n",
      " [    0.             0.             0.             0.\n",
      "  41787.16091688     0.         48536.91803362     0.\n",
      "  43116.48192446]\n",
      " [    0.             0.             0.             0.\n",
      "      0.             0.             0.         46807.93679676\n",
      "      0.        ]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "qagent_rewards = QAgent(alpha, gamma, epsilon, location_to_state, actions, rewards_modified, state_to_location)\n",
    "print(\"\\n--- Training with Modified Rewards (L1-L2 more rewarding) ---\")\n",
    "qagent_rewards.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1404e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Modified Rewards (L1-L2 more rewarding) ---\n",
      "[[  0   5   0   0   0   0   0   0   0]\n",
      " [ 20   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[   0.         1415.67853996    0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [ 724.94515868    0.          385.26815086    0.         1702.19305335\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.         1119.95775277    0.            0.            0.\n",
      "   513.72578914    0.            0.            0.        ]\n",
      " [   0.            0.            0.         5726.37377768    0.\n",
      "     0.         3691.33198619    0.            0.        ]\n",
      " [   0.         1073.90689148    0.            0.            0.\n",
      "     0.            0.         2468.90200325    0.        ]\n",
      " [   0.            0.          777.26599734    0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.         4949.6397219     0.\n",
      "     0.            0.         2284.68622637    0.        ]\n",
      " [   0.            0.            0.            0.         1414.70757056\n",
      "     0.         3293.11547346    0.         1650.07360101]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.         2851.79356353    0.        ]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "qagent_rewards = QAgent(0.1, 0.99, 0.99, location_to_state, actions, rewards_modified, state_to_location)\n",
    "print(\"\\n--- Training with Modified Rewards (L1-L2 more rewarding) ---\")\n",
    "qagent_rewards.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364c1e6",
   "metadata": {},
   "source": [
    "### Conclusion from Final Training Run\n",
    "\n",
    "I was curious about whether the agent will select a much more different route when I give a really high reward for a specific location. \n",
    "So I adjusted the hyperparameters in a way that makes exploration important: (`alpha = 0.1`, `gamma = 0.99`, `epsilon = 0.99`) and increased the reward for the transition from `L1` to `L2` (`rewards_modified[0, 1] = 5`) and significantly increased the reward for the transition from `L2` to `L1` (`rewards_modified[1, 0] = 20`).\n",
    "Specifically the combination of a very high epsilon and the high reward to go from `L1` to `L2` made me hope for altering the optimal route.\n",
    "\n",
    "But given the structure of the environment, the optimal route from `L9` to `L4` is typically `L9 -> L8 -> L7 -> L4`. This path does not directly involve locations `L1` or `L2`. Therefore, even with a very high reward for the `L2 -> L1` transition, it is *unlikely* that the optimal path to `L4` would change unless `L1` or `L2` were to become an advantageous intermediate step leading to `L4`.\n",
    "\n",
    "The chosen hyperparameters (`alpha=0.1` for stable learning, `gamma=0.99` for valuing future rewards, and `epsilon=0.99` for a very preferred exploration) contribute to a robust learning process. However, the agent's behavior is fundamentally constrained by the environment's topology as defined by the rewards matrix.\n",
    "\n",
    "**Conclusion**:\n",
    "The high reward for `L2 -> L1` primarily influences paths that directly utilize this transition. For a goal like `L4`, which is not directly connected or efficiently reachable via `L1` or `L2` within the current environment, this specific reward modification may not alter the established optimal route. This emphasizes that while hyperparameters and individual rewards are critical, the overall connectivity and reward structure of the environment ultimately dictate the optimal strategies learned by the agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c425f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
