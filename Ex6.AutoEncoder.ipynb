{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c76ea3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9505dc0",
   "metadata": {},
   "source": [
    "# 6th exercise: <font color=\"#C70039\">Work with Autoencoders for anomaly detection</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   04.08.2025\n",
    "\n",
    "<img src=\"https://blog.keras.io/img/ae/autoencoder_schema.jpg\" style=\"float: center;\" width=\"700\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "Autoencoder is an unsupervised artificial neural network (ANN) that learns how to efficiently compress and encode data and then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "An Autoencoder reduces data dimensions by learning how to ignore the noise in the data and thus outliers.\n",
    "In the section above, you can seen an example of the input/output image from the MNIST dataset to an Autoencoder.\n",
    "\n",
    "#### Autoencoder Components:\n",
    "An Autoencoder consists of four main parts:\n",
    "\n",
    "1. Encoder: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.\n",
    "\n",
    "2. Bottleneck: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data. The bottlneck is also called latent vector. The concept of the latent space and latent vectors becomes important later on as we move forward to understand Generative Models. \n",
    "\n",
    "3. Decoder: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.\n",
    "\n",
    "4. Reconstruction Loss: This is the method that measures how well the decoder is performing and how close the output is to the original input.\n",
    "\n",
    "As always in ANNs, the training itself involves back propagation in order to minimize the network’s reconstruction loss.\n",
    "\n",
    "Due to this features of an Autoencoder the use cases are manyfold. One of the obviously is anomaly detection. \n",
    "\n",
    "#### Autoencoder Architecture:\n",
    "\n",
    "The network architecture for Autoencoders can vary between simple Feed Forward networks, Recurrent Neural Networks (LSTM) or Convolutional Neural Networks (CNN) depending on the use case. \n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time.\n",
    "    * the example below shows how to use an autoencoder for anomaly detection\n",
    "\n",
    "4. <font color=green>Develop an Autoencoder for Domain Adaptation (Me -> Walter White ). You can of course also take own data, e.g. a photo of yours and someone else.</font>\n",
    "5. Set at least the following hyperparameters for training (epochs=100000, shuffle=True).\n",
    "6. Implement a CNN for working out important features for the adaptation. If you feel lost in the exercise, please visit the sample solution.\n",
    "7. There is also an implementation of data augmentation that helps you building up your data set from one single \"original\" image. \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ff998",
   "metadata": {},
   "source": [
    "### Auto-Encoding\n",
    "If you have correlated input data, the auto-encoder method will work very well because the encoding operation relies on the correlated features to compress the data.\n",
    "Let’s consider that an auto-encoder is trained on the MNIST dataset. \n",
    "As you know already, using a simple FeedForward neural network, this can be done by building a simple 6 layers network as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from   keras.datasets import mnist\n",
    "from   keras.models import Sequential, Model\n",
    "from   keras.layers import Dense, Input\n",
    "from   keras import optimizers\n",
    "from   keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5a017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Autoencoder Training...\n",
      "Epoch 1/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0756 - val_loss: 0.0501\n",
      "Epoch 2/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0435 - val_loss: 0.0367\n",
      "Epoch 3/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0334 - val_loss: 0.0297\n",
      "Epoch 4/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - loss: 0.0286 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0261 - val_loss: 0.0246\n",
      "Epoch 6/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0244 - val_loss: 0.0232\n",
      "Epoch 7/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0231 - val_loss: 0.0222\n",
      "Epoch 8/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0222 - val_loss: 0.0213\n",
      "Epoch 9/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0214 - val_loss: 0.0206\n",
      "Epoch 10/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.0207 - val_loss: 0.0200\n",
      "Training complete.\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Shape of Encoded Data (Latent Vectors): (60000, 10)\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "Decoder model successfully extracted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_tensor_61CLONE            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">402,192</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_tensor_61CLONE            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │       \u001b[38;5;34m402,192\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">469,648</span> (1.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m469,648\u001b[0m (1.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">469,648</span> (1.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m469,648\u001b[0m (1.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_x = x_train.reshape(60000, 784) / 255\n",
    "val_x = x_test.reshape(10000, 784) / 255\n",
    "\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoded = Dense(512, activation='elu')(input_img)\n",
    "encoded = Dense(128, activation='elu')(encoded)\n",
    "bottleneck = Dense(10, activation='linear', name=\"bottleneck\")(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='elu')(bottleneck)\n",
    "decoded = Dense(512, activation='elu')(decoded)\n",
    "final_output_tensor = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, final_output_tensor)\n",
    "\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "\n",
    "\n",
    "print(\"Starting Autoencoder Training...\")\n",
    "trained_model = autoencoder.fit(\n",
    "    train_x, \n",
    "    train_x, \n",
    "    batch_size=1024, \n",
    "    epochs=10, \n",
    "    verbose=1, \n",
    "    validation_data=(val_x, val_x)\n",
    ")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "encoder = Model(input_img, bottleneck) \n",
    "\n",
    "encoded_data = encoder.predict(train_x) \n",
    "print(f\"Shape of Encoded Data (Latent Vectors): {encoded_data.shape}\")\n",
    "\n",
    "reconstructed_images = autoencoder.predict(train_x) \n",
    "\n",
    "decoder = Model(bottleneck, final_output_tensor)\n",
    "\n",
    "print(f\"Decoder model successfully extracted.\")\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b24e1e",
   "metadata": {},
   "source": [
    "### Heisenberg Autoencoder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c0a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.0758 - val_loss: 0.0495\n",
      "Epoch 2/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0429 - val_loss: 0.0369\n",
      "Epoch 3/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0334 - val_loss: 0.0298\n",
      "Epoch 4/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0286 - val_loss: 0.0265\n",
      "Epoch 5/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0259 - val_loss: 0.0243\n",
      "Epoch 6/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0239 - val_loss: 0.0227\n",
      "Epoch 7/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0226 - val_loss: 0.0216\n",
      "Epoch 8/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0216 - val_loss: 0.0207\n",
      "Epoch 9/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0207 - val_loss: 0.0199\n",
      "Epoch 10/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0200 - val_loss: 0.0193\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All `outputs` values must be KerasTensors. Received: outputs=[[1.11649825e-07 3.48466492e-05 4.04896891e-05 ... 1.69892610e-05\n  2.89210038e-06 3.25315305e-05]\n [3.51397721e-05 5.63599024e-05 6.82316197e-04 ... 1.46081557e-05\n  1.59327334e-04 5.93641947e-04]\n [3.05072088e-02 2.44151405e-03 1.46244904e-02 ... 1.70766469e-02\n  1.44277085e-02 3.89655097e-03]\n ...\n [5.04813170e-06 4.30117097e-06 8.47499250e-05 ... 3.94409326e-05\n  1.17129039e-05 3.69951704e-05]\n [1.65066904e-05 4.51316264e-05 1.96839173e-04 ... 2.02047031e-05\n  2.25126605e-05 4.97998495e-04]\n [3.78641387e-04 5.57963038e-04 8.86792841e-04 ... 1.39388384e-03\n  1.73206892e-04 2.44959723e-04]] including invalid value [[1.11649825e-07 3.48466492e-05 4.04896891e-05 ... 1.69892610e-05\n  2.89210038e-06 3.25315305e-05]\n [3.51397721e-05 5.63599024e-05 6.82316197e-04 ... 1.46081557e-05\n  1.59327334e-04 5.93641947e-04]\n [3.05072088e-02 2.44151405e-03 1.46244904e-02 ... 1.70766469e-02\n  1.44277085e-02 3.89655097e-03]\n ...\n [5.04813170e-06 4.30117097e-06 8.47499250e-05 ... 3.94409326e-05\n  1.17129039e-05 3.69951704e-05]\n [1.65066904e-05 4.51316264e-05 1.96839173e-04 ... 2.02047031e-05\n  2.25126605e-05 4.97998495e-04]\n [3.78641387e-04 5.57963038e-04 8.86792841e-04 ... 1.39388384e-03\n  1.73206892e-04 2.44959723e-04]] of type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     89\u001b[39m decoded_output = autoencoder.predict(train_x) \u001b[38;5;66;03m# reconstruction\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 2. Extract the DECODER Model\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# The Decoder starts at the bottleneck tensor and ends at the final output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m decoder = \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nutzer\\Documents\\uni-stuff\\AML\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tracking.py:26\u001b[39m, in \u001b[36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nutzer\\Documents\\uni-stuff\\AML\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:127\u001b[39m, in \u001b[36mFunctional.__init__\u001b[39m\u001b[34m(self, inputs, outputs, name, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_outputs:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend.KerasTensor):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    128\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAll `outputs` values must be KerasTensors. Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         )\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_input_keras_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_inputs):\n\u001b[32m    134\u001b[39m     inputs, outputs = clone_graph_nodes(inputs, outputs)\n",
      "\u001b[31mValueError\u001b[39m: All `outputs` values must be KerasTensors. Received: outputs=[[1.11649825e-07 3.48466492e-05 4.04896891e-05 ... 1.69892610e-05\n  2.89210038e-06 3.25315305e-05]\n [3.51397721e-05 5.63599024e-05 6.82316197e-04 ... 1.46081557e-05\n  1.59327334e-04 5.93641947e-04]\n [3.05072088e-02 2.44151405e-03 1.46244904e-02 ... 1.70766469e-02\n  1.44277085e-02 3.89655097e-03]\n ...\n [5.04813170e-06 4.30117097e-06 8.47499250e-05 ... 3.94409326e-05\n  1.17129039e-05 3.69951704e-05]\n [1.65066904e-05 4.51316264e-05 1.96839173e-04 ... 2.02047031e-05\n  2.25126605e-05 4.97998495e-04]\n [3.78641387e-04 5.57963038e-04 8.86792841e-04 ... 1.39388384e-03\n  1.73206892e-04 2.44959723e-04]] including invalid value [[1.11649825e-07 3.48466492e-05 4.04896891e-05 ... 1.69892610e-05\n  2.89210038e-06 3.25315305e-05]\n [3.51397721e-05 5.63599024e-05 6.82316197e-04 ... 1.46081557e-05\n  1.59327334e-04 5.93641947e-04]\n [3.05072088e-02 2.44151405e-03 1.46244904e-02 ... 1.70766469e-02\n  1.44277085e-02 3.89655097e-03]\n ...\n [5.04813170e-06 4.30117097e-06 8.47499250e-05 ... 3.94409326e-05\n  1.17129039e-05 3.69951704e-05]\n [1.65066904e-05 4.51316264e-05 1.96839173e-04 ... 2.02047031e-05\n  2.25126605e-05 4.97998495e-04]\n [3.78641387e-04 5.57963038e-04 8.86792841e-04 ... 1.39388384e-03\n  1.73206892e-04 2.44959723e-04]] of type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# load the inbuild mnist data set (8bit grayscale digits)\n",
    "# https://en.wikipedia.org/wiki/MNIST_database\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# normalize the training and the validation data set\n",
    "train_x = x_train.reshape(60000, 784) / 255\n",
    "val_x = x_test.reshape(10000, 784) / 255\n",
    "'''\n",
    "\n",
    "\n",
    "# build the auto-encoding layers\n",
    "'''\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(Dense(512,  activation='elu', input_shape=(784,)))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(10,   activation='linear', name=\"bottleneck\"))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(512,  activation='elu'))\n",
    "autoencoder.add(Dense(784,  activation='sigmoid'))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "'''\n",
    "'''\n",
    "NOTE:\n",
    "-----\n",
    "The Exponential Linear Unit (ELU) is an activation function for neural networks. \n",
    "In contrast to ReLUs (which you know), ELUs have negative values which allows them to push mean unit \n",
    "activations closer to zero like batch normalization but with lower computational complexity.\n",
    "'''    \n",
    "\n",
    "# train the model and finally assign the encoding to the decoder\n",
    "'''\n",
    "NOTE:\n",
    "-----\n",
    "make sure you understand, that you are training on train_x and not on train_y but train_x again for the reconstruction\n",
    "the same for the validation (val_x, val_x)\n",
    "'''\n",
    "'''\n",
    "trained_model = autoencoder.fit(train_x, train_x, batch_size=1024, epochs=5, verbose=1, validation_data=(val_x, val_x))\n",
    "encoder = Model(autoencoder.input, autoencoder.get_layer('bottleneck').output)\n",
    "\n",
    "encoded_data = encoder.predict(train_x)  # bottleneck representation\n",
    "\n",
    "decoded_output = autoencoder.predict(train_x)        # reconstruction\n",
    "encoding_dim = 10\n",
    "\n",
    "# return the decoder\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder = autoencoder.layers[-3](encoded_input)\n",
    "decoder = autoencoder.layers[-2](decoder)\n",
    "decoder = autoencoder.layers[-1](decoder)\n",
    "decoder = Model(encoded_input, decoder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b584c79",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b39fb",
   "metadata": {},
   "source": [
    "As you can see in the output, the last reconstruction loss/error for the validation set is approx. 0.0197, which is great. \n",
    "Now, if we pass any normal image from the MNIST dataset, the reconstruction loss will be very low (< 0.02) BUT if we tried to pass any other different image (outlier / anomaly), we will get a high reconstruction loss value because the network failed to reconstruct the image/input that is considered an anomaly.\n",
    "\n",
    "Notice in the code above, you can use only the encoder part to compress some data or images and you can also only use the decoder part to decompress the data by loading the decoder layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2edde3",
   "metadata": {},
   "source": [
    "## Auto-Encoders for Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf39053",
   "metadata": {},
   "source": [
    "Now, let’s do some anomaly detection. The code below uses two different images to predict the anomaly score (reconstruction error) using the autoencoder network we trained above. \n",
    "\n",
    "The first image is from the MNIST and the result is error=2.46241018. This means that the image is not an anomaly. The second image (yoda.png) obviously does not belong to the training dataset and the result is: error=2727.0718. This high error means that the image is an anomaly. Even the third image. The same concept applies to any type of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9e8232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "[2.87073906]\n"
     ]
    }
   ],
   "source": [
    "# If you are using a newer version of keras than '2.4.3', read this article below.\n",
    "# It describes a versioning issue in the keras libs\n",
    "'''https://stackoverflow.com/questions/72383347/how-to-fix-it-attributeerror-module-keras-preprocessing-image-has-no-attribu'''\n",
    "#from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "# take an image from the validation data set or the training data set, respectively\n",
    "input_img = val_x[50] \n",
    "input_img_flat = input_img.reshape(1,784)\n",
    "\n",
    "target_data = autoencoder.predict(input_img_flat)\n",
    "\n",
    "dist = np.linalg.norm(input_img_flat - target_data, axis=-1)\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccafc75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[2728.0059]\n"
     ]
    }
   ],
   "source": [
    "# Now take Master Yoda as the test image. The error score will be very high (error=2727.0718)\n",
    "img = load_img(\"./data/yoda.png\", target_size=(28, 28), color_mode = \"grayscale\")\n",
    "input_img = img_to_array(img)\n",
    "\n",
    "input_img_flat = input_img.reshape(1,784)\n",
    "target_data = autoencoder.predict(input_img_flat)\n",
    "dist = np.linalg.norm(input_img_flat - target_data, axis=-1)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b553f742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "[2551.9473]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Now take a Mnist image which is taken from the google image search and although it is super similar to the training data\n",
    "it does not belong to the same data distribution the auto-encoder was trained on. \n",
    "It produces an error almost as high as yoda.png (approx error=2551.99)\n",
    "This makes autoencoders being a very robust technique for anomaly detection.\n",
    "'''\n",
    "img = load_img(\"./data/similarMnistNumber.jpg\", target_size=(28, 28), color_mode = \"grayscale\")\n",
    "input_img = img_to_array(img)\n",
    "\n",
    "input_img_flat = input_img.reshape(1,784)\n",
    "target_data = autoencoder.predict(input_img_flat)\n",
    "dist = np.linalg.norm(input_img_flat - target_data, axis=-1)\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ed792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
